{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf86f9d-063d-4cc8-bd25-19d9bcda2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99592670-70e6-409f-84aa-02bb753e4447",
   "metadata": {},
   "source": [
    "## Table 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b40cc9-6127-44e7-904b-b98dca49e72e",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6281930f-5ae8-4384-a5d4-595f699f566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2 = pd.DataFrame(\n",
    "    index=pd.Series(\n",
    "        [\"NYT\", \"WikiText\", \"Goodreads (Romance)\", \"Goodreads (History/Biography)\"],\n",
    "        name=\"Dataset\",\n",
    "    ),\n",
    "    columns=[\n",
    "        \"Total Documents\",\n",
    "        \"Total Words\",\n",
    "        \"Vocabulary Size\",\n",
    "        \"Mean Document Length\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c863e9-339e-475a-84a9-b629eb8d4a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Documents</th>\n",
       "      <th>Total Words</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Mean Document Length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NYT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WikiText</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goodreads (Romance)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goodreads (History/Biography)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Total Documents Total Words Vocabulary Size  \\\n",
       "Dataset                                                                     \n",
       "NYT                                       NaN         NaN             NaN   \n",
       "WikiText                                  NaN         NaN             NaN   \n",
       "Goodreads (Romance)                       NaN         NaN             NaN   \n",
       "Goodreads (History/Biography)             NaN         NaN             NaN   \n",
       "\n",
       "                              Mean Document Length  \n",
       "Dataset                                             \n",
       "NYT                                            NaN  \n",
       "WikiText                                       NaN  \n",
       "Goodreads (Romance)                            NaN  \n",
       "Goodreads (History/Biography)                  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15509f51-cbe6-49a1-90bb-0159c54e54aa",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "If you already have saved proprocessed files, can skip preprocessing and just load them from disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4005950-7e95-4706-be0b-3d064a15b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer only\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\",\n",
    "    disable=[\"tok2vec\", \"tagger\", \"ner\", \"lemmatizer\", \"parser\", \"attribute_ruler\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7a049d-c6f9-4e21-a3eb-1ddd77d2f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"../data/WikiText103/wikitext-103/wiki.train.tokens\"\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "# remove lines with formulas\n",
    "lines = [line for line in lines if line != \" <formula> \\n\"]\n",
    "# determine document start indexes using combination of regex patterns\n",
    "title_regex = re.compile(\" = .* = \\n\")\n",
    "subtitle_regex = re.compile(\" = = .* = = \\n\")\n",
    "new_line_regex = re.compile(\" \\n\")\n",
    "doc_idxs = [\n",
    "    idx\n",
    "    for idx, (prev_line, cur_line, next_line) in enumerate(\n",
    "        zip(lines[:-1], lines[1:], lines[2:]), 1\n",
    "    )\n",
    "    if new_line_regex.match(prev_line)\n",
    "    and new_line_regex.match(next_line)\n",
    "    and title_regex.match(cur_line)\n",
    "    and not subtitle_regex.match(cur_line)\n",
    "] + [len(lines) - 1]\n",
    "# split/concat lines into docs; lowercase; remove newlines and formulas\n",
    "documents = [\n",
    "    \"\".join(lines[start_idx:end_idx])\n",
    "    # .replace(\"\\n\", \"\")\n",
    "    # .replace(\"<formula>\", \"\")\n",
    "    .strip()\n",
    "    .lower()\n",
    "    for start_idx, end_idx in zip(doc_idxs, doc_idxs[1:])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508d6253-3e06-43a4-9b6b-67bee53b3c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 28472/28472 [03:20<00:00, 141.84it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_docs = [nlp(doc) for doc in tqdm(documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de7875-afe4-406d-bba8-de648706a593",
   "metadata": {},
   "source": [
    "### WikiText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba1708e-da15-4adf-b496-e83924c12110",
   "metadata": {},
   "source": [
    "#### Total Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad9a7e78-7905-4a22-8631-2fc0b0c837ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2.loc[\"WikiText\"][\"Total Documents\"] = len(wiki_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e3b9d-69ed-4912-a44f-a908a6018e4a",
   "metadata": {},
   "source": [
    "#### Total Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5806a3c-d7da-491e-b1a9-70157127357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 28472/28472 [00:22<00:00, 1248.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85327167"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many alphanumeric tokens\n",
    "np.sum(\n",
    "    [\n",
    "        np.sum(doc.to_array([\"IS_ALPHA\", \"IS_DIGIT\"]).any(axis=1))\n",
    "        for doc in tqdm(wiki_docs)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d7070-39fb-4a38-a4f9-566e410e1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "85325590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14cbf362-664e-4e9b-af9c-dfc94c2aaa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 28472/28472 [00:00<00:00, 748575.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103620897"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just every token\n",
    "np.sum(\n",
    "    [\n",
    "        len(doc)\n",
    "        for doc in tqdm(wiki_docs)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef63899-56a5-4ada-993d-9434bcb545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weird, should be getting 99 mil at some point..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
